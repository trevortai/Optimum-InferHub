# Optimum-InferHub
Using Optimum to run inference on a variety of execution providers for ONNX models


To start, you will need an onnx model

If you do not have one, you can find models from hugging face and run the convert script from optimum on it if it is not already onnx.

run ```python read.py with --model localmodel --audio audiofile --timer``` for a whisper example
